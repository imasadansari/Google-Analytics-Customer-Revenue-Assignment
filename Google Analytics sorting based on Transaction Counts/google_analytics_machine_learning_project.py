# -*- coding: utf-8 -*-
"""Google Analytics Machine Learning Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Vt8R9N9SRHVlBFs_jE2k45Hdg7taYNL

### Initiate Notebook
"""

! pip install -q kaggle

from google.colab import files
files.upload()

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c iiitb-ga-customer-revenue-prediction -p /content

!unzip -u \*.zip

! find . -name "*.zip" -type f -delete

"""## Import Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json 
from datetime import date
from pandas.io.json import json_normalize
from sklearn.model_selection import KFold, StratifiedKFold
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GroupKFold
from sklearn.ensemble import RandomForestRegressor 
import gc

# %matplotlib inline

"""# Import Dataset Test and Train"""

#Split json columns as maincolumn_subcolumn

json_columns = ['device', 'geoNetwork','totals', 'trafficSource']
def load_dataframe(filename):
    #path = "../input/" + filename
    df = pd.read_csv(filename, converters={column: json.loads for column in json_columns}, 
                     dtype={'fullVisitorId': 'str'})
    
    for column in json_columns:
        column_as_df = json_normalize(df[column])
        column_as_df.columns = [f"{column}_{subcolumn}" for subcolumn in column_as_df.columns]
        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)
    return df

#Load Test Dataset
test = load_dataframe("ga_testV2.csv")
test.info()

#View Test Dataset
test = pd.DataFrame(test)
test.head(10)

#Load Train Dataset and Viewing
train = load_dataframe("ga_trainV2.csv")
train = pd.DataFrame(train)

#View Train Dataset
train.head(10)

train.info()

test.info()
# Test Dataset Doesn't have 'trafficSource_campainCode' column

"""## Drop columns not in Test Dataset but are present in Trainning Dataset"""

unwanted_col = set(train.columns).difference(set(test.columns))
for col in unwanted_col:
  del train[col]
unwanted_col

print('Test : ',test.shape)
print('Train : ', train.shape)

"""## Sepreate out numerical and categorical columns"""

# For Training Dataset 
# Seperate out numerical and object coulmns
numerical_features_train = train.select_dtypes(include = [np.number])
print(numerical_features_train.columns,  numerical_features_train.shape)

categorical_features_train = train.select_dtypes(include = [np.object])
print(categorical_features_train.columns,  categorical_features_train.shape)

#Same  Seperation for Test Dataset
numerical_features_test = test.select_dtypes(include = [np.number])
print(numerical_features_test.columns,  numerical_features_test.shape)

categorical_features_test = test.select_dtypes(include = [np.object])
print(categorical_features_test.columns, categorical_features_test.shape)

"""## Remove columns which have only one value through out column"""

# We have some columns which have same value for whole dataset
# Like in geo location we have value "not available in demoset"
# Also if value is common it doesn't contribute anything
# Considering that test dataset also have same conditions

#Removing Const columns
column_names = train.columns.to_list()
columns_with_one_value = []  
#train adCampainCode
for col in column_names:
  count = train[col].nunique(dropna = False)
  if count == 1 and col not in ['totals_bounces', 'totals_hits', 'totals_newVisits']:
    #This column has only one value
    del train[col]
    if col in test.columns.to_list():#For adcampaingcolumn 
      del test[col]
    #Is this need to be done with test
    columns_with_one_value.append(col)

columns_with_one_value

print(test.shape, train.shape)

train.columns

test.columns

"""## Find columns which have missing values and their percentage"""

#Lets start with missing value treatment then move on to Exploratory analysis Analysis
#First for categorical features
#For Train 
all_missing_value_test = categorical_features_train.isnull().sum().sort_values(ascending = False)
#Calculating percentage
percent = (categorical_features_train.isnull().sum() / categorical_features_train.isnull().count()).sort_values(ascending = False) * 100
missing_data = pd.concat([all_missing_value_test, percent], axis=1,join='outer', keys=['Total Missing Count', ' % of Total Observations'])
missing_data.index.name ='Feature'
missing_data.head(15)
#NoT taking eda of test as it is similar to train

#Visualizing
missing_values = categorical_features_train.isnull().sum(axis=0).reset_index()
missing_values.columns = ['column_name', 'missing_count']
missing_values = missing_values.loc[missing_values['missing_count']>0]
missing_values = missing_values.sort_values(by='missing_count')
indx = np.arange(missing_values.shape[0])
width = 0.2
fig, ax = plt.subplots(figsize=(12,3))
rects = ax.barh(indx, missing_values.missing_count.values, color='b')
ax.set_yticks(indx)
ax.set_yticklabels(missing_values.column_name.values, rotation='horizontal')
ax.set_xlabel("Missing Observations Count")
ax.set_title("Missing Categorical Observations in Train Dataset")
plt.show()

#for col in train.columns:
 # print(col, train[col].unique())

"""All Columns Treatment one by one

# Exploratory data analysis of each column

## Revenue Generating Customers
"""

train["totals_transactionRevenue"] = train["totals_transactionRevenue"].astype('float')
gdf = train.groupby("fullVisitorId")["totals_transactionRevenue"].sum().reset_index()

nzc = pd.notnull(train["totals_transactionRevenue"]).sum()
nzr = (gdf["totals_transactionRevenue"]>0).sum()
print("Number of customers in train set with non-zero revenue : ", nzc, " and ratio is : ", nzc / train.shape[0])
print("Number of unique customers with non-zero revenue : ", nzr, "and the ratio is : ", nzr / gdf.shape[0])

"""## Number of Customers common in both Training and Test Set"""

print("Number of unique visitors in train set : ",train.fullVisitorId.nunique(), " out of rows : ",train.shape[0])
print("Number of unique visitors in test set : ",test.fullVisitorId.nunique(), " out of rows : ",test.shape[0])
print("Number of common visitors in train and test set : ",len(set(train.fullVisitorId.unique()).intersection(set(test.fullVisitorId.unique())) ))

"""So we are dealing with completely new customers

## Fill null tansaction value with 0
"""

#Fill total transaction revenue of null as 0
train['totals_transactionRevenue'] = train['totals_transactionRevenue'].fillna(0)
test['totals_transactionRevenue'] = test['totals_transactionRevenue'].fillna(0)

"""## Target Feature Analysis"""

train["totals_transactionRevenue"] = train["totals_transactionRevenue"].astype('float')
gdf = train.groupby("fullVisitorId")["totals_transactionRevenue"].sum().reset_index()

plt.figure(figsize=(8,6))
#plt.hist( gdf["totals.transactionRevenue"].values)
plt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf["totals_transactionRevenue"].values)))
plt.xlabel('index', fontsize=12)
plt.ylabel('TransactionRevenue', fontsize=12)
plt.show()

"""So we are dealing with completely skewded data.
As the description says we have 20% revenue generating customers

## Date and VisitStartTime
"""

#library of datetime
from datetime import datetime

# This function is to extract date features
def date_process(df):
    df["date"] = pd.to_datetime(df["date"], format="%Y%m%d") # seting the column as pandas datetime
    df["_weekday"] = df['date'].dt.dayofweek #extracting week day
    df["_day"] = df['date'].dt.day # extracting day
    df["_month"] = df['date'].dt.month # extracting month
    df["_year"] = df['date'].dt.year # extracting year
    df['_visitHour'] = (df['visitStartTime'].apply(lambda x: str(datetime.fromtimestamp(x).hour))).astype(int)
    return df #returning the df after the transformations

train.head(10)

train_date = date_process(train)
test_date = date_process(test)

train_date.head(100000)

"""### Exploring Date Field """

#convert total_transaction revenue to 'float64' from  'object'
train_date = train_date.astype({'totals_transactionRevenue': 'float64'}, copy = True)
test_date = test_date.astype({'totals_transactionRevenue': 'float64'}, copy = True)

train_date.info()

train_date.head()

"""### Check impack of Visit Hour or time of visit on Transaction Revenue"""

grpbyvisitHour = train_date.groupby('_visitHour')
revenue = dict(grpbyvisitHour['totals_transactionRevenue'].sum())
plt.bar(revenue.keys() , revenue.values())

"""This clearly shows there is more sales during from 14 to 23 hrs

### Impact of Weekday
"""

grpbyweek = train_date.groupby('_weekday')
revenue = dict(grpbyweek['totals_transactionRevenue'].sum())
plt.bar(revenue.keys() , revenue.values())

"""On weekends Sales is less while on weekdays it is more

### Lets analyze revenue on 'day' basis
"""

grpbyday = train_date.groupby('_day')
revenue = dict(grpbyday['totals_transactionRevenue'].sum())
plt.bar(revenue.keys() , revenue.values())

"""There doesnt seem much dependence on day. but a bit can be said that during month ends sales get stable while there is both in moth start

### Lets analyze on 'month' basis
"""

grpbymonth = train_date.groupby('_month')
revenue = dict(grpbymonth['totals_transactionRevenue'].sum())
plt.bar(revenue.keys() , revenue.values())

"""Things seems similar for month. Just during year end sales increase a bit

Check for 'Year'
"""

grpbyyear = train_date.groupby('_year')
revenue = dict(grpbyyear['totals_transactionRevenue'].sum())
plt.bar(revenue.keys() , revenue.values())

"""We can keep this rise """

print(train_date['date'].min(), train_date['date'].max())

train_date['date'].describe()

print(test_date['date'].min(), test_date['date'].max())

test_date['date'].describe()

"""So in both of our datasets will have same date range

### Date vs Total revenue
"""

#Train Dataset
plt.figure(figsize=(12, 8))
sns.lineplot(train_date['date'], train_date['totals_transactionRevenue'])

"""We can see on some days, there is **Sale Spike** alot Like in November-2016, February end - March-2017, April 2017

### Date vs userCounts in train set
"""

cnt_urs = train_date.groupby('date')['fullVisitorId'].size().reset_index()
plt.figure(figsize=(12, 8))
sns.lineplot(cnt_urs['date'], cnt_urs['fullVisitorId'])

"""As we can see During November December 2016, and during May 2017 user mostly visit Google store.

## Analysis of Channel Grouping
"""

print('channelGrouping values')
print(train['channelGrouping'].unique())

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.figure(figsize=(10,5))
sns.set_theme(style="darkgrid")
ax = sns.countplot(x="channelGrouping", data=train)

grpbychannel = train_date.groupby('channelGrouping')
revenue = dict(grpbychannel['totals_transactionRevenue'].sum())
plt.figure(figsize= (10,10))
plt.bar(revenue.keys() , revenue.values())

"""## Function to extract column specific category"""

import re
def columns_extract(category):
    cat_cols = list()
    for i in train_date.columns: 
        a = re.findall(r'^'+category+'.*',i)
        if a:
            cat_cols.append(a[0])
        else:
            continue
    return cat_cols

'''
Function to plot 4 types of barplots for each categorical column
1) Categories vs their counts
2) Categories vs Non-Zero Revenue
3) Categories vs Mean Transaction Revenue
4) Categories vs Total Reveue
'''
def category_plots(col):
    a = train_date.loc[:,[col, 'totals_transactionRevenue']]
    a['totals_transactionRevenue'] = a['totals_transactionRevenue'].replace(0.0, np.nan)
    #a['totals_transactionRevenue'] = a['totals_transactionRevenue'].apply(np.expm1)
    cnt_srs = a.groupby(col)['totals_transactionRevenue'].agg(['size','count','mean'])
    cnt_srs.columns = ["count", 'count of non-zero revenue', "mean transaction value"]

    cnt_srs['total_revenue'] = cnt_srs['count of non-zero revenue']*cnt_srs['mean transaction value']
    cnt_srs = cnt_srs.sort_values(by="count", ascending=False)
    print(cnt_srs.head(10))
    plt.figure(figsize=(8, 20)) 
    plt.subplot(4,1,1)
    sns.barplot(x=cnt_srs['count'].head(10), y=cnt_srs.index[:10])
    plt.subplot(4,1,2)
    sns.barplot(x=cnt_srs['count of non-zero revenue'].head(10), y=cnt_srs.index[:10])
    plt.subplot(4,1,3)
    sns.barplot(x=cnt_srs['mean transaction value'].head(10), y=cnt_srs.index[:10])
    plt.subplot(4,1,4)
    sns.barplot(x=cnt_srs['total_revenue'].head(10), y=cnt_srs.index[:10])

#cnt_srs

"""## Lets Explore Device column and sub columns"""

device_columns = columns_extract('device')

train_date[device_columns].nunique(dropna = False)

"""### Device Browser"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
category_plots('device_browser')

train_date['device_browser'].unique()

'''
Function to reduce categorical values in browser: Here we are keeping only popular browsers abd merging some not much known browsers
'''
def popular_browser(x):
    browsers = ['chrome','safari','firefox','internet explorer','edge','opera', 'opera mini','uc browser', 'coc coc','iron', 
                'android webview', 'safari (in-app)', 'puffin', 'yabrowser', 'amazon silk' ]
    if x in browsers:
        return x.lower()
    elif 'android' in x:
      return 'android'
    elif 'browser' in x:
      return 'browser'
    elif 'mozilla' in x:
      return 'mozilla'  
    elif 'chrome' in x:
      return 'dupchrome'
    elif 'nokia' in x:
      return 'nokia'
    elif '(not set)' in x or 'nan' in x:
        return x
    else:
        return 'unpopular_browser'

#Keeping popular browsers in both Test and Train dataset
train_date['device_browser'] = train_date['device_browser'].apply(lambda x: popular_browser(str(x).lower()))
test_date['device_browser'] = test_date['device_browser'].apply(lambda x: popular_browser(str(x).lower()))

train_date['device_browser'].nunique()

train_date['device_browser'].unique()

"""### Work on Device operating system"""

train_date['device_operatingSystem'].unique()

category_plots('device_operatingSystem')

'''
Function to reduce categorical values in Oprating System: Here we are keeping only popular browsers abd merging some not much known browsers
'''
def popular_OS(x):
    OS = ['Macintosh', 'Windows', 'Android', 'iOS', 'Linux', 'Chrome OS','windows phone', 'Samsung', 'Xbox']
    if x in OS:
        return x.lower()
    elif 'Nintendo' in x:
      return 'Nintendo'
    elif '(not set)' in x or 'nan' in x:
        return x
    else:
        return 'unpopular_OS'

# Make Changes in both Train and Test Dataset
train_date['device_operatingSystem'] = train_date['device_operatingSystem'].apply(lambda x: popular_OS(x))
test_date['device_operatingSystem'] = test_date['device_operatingSystem'].apply(lambda x: popular_OS(x))

train_date['device_operatingSystem'].unique()

"""### Work on device category"""

category_plots('device_deviceCategory')

"""Desktop users are more and do generate more revenue
In case of tablet and mobile users Mobile users are more
but both have comparable revenue

### Convert isMobile to bool from string
"""

# here we are converting the "device.isMobile" data type from string to boolean.
train_date['device_isMobile']=   train_date['device_isMobile'].astype(bool)
test_date['device_isMobile']  = test_date['device_isMobile'].astype(bool)

"""## **Explore GeoNetwork Columns**"""

geo_cols = columns_extract('geoNetwork')
geo_cols

train_date[geo_cols].nunique(dropna = False)

"""### GeoNetwork Continents

"""

category_plots('geoNetwork_continent')

"""
*   Though Asia and Europe has large number of users but revenue genetation is alot less
*   Americans are by far highest revenue Generators
*   Africans are less users but they have notably large mean transaction value
*   We can merge Oceanic and not set as both are similar



"""

# Here we are just changing '(not set)' to 'UnknownContinent' for both test and training dataset
less_continent = ['(not set)']
train_date['geoNetwork_continent'] = train_date['geoNetwork_continent'].apply(lambda x: x if x not in less_continent else 'UnknownContinent')
test_date['geoNetwork_continent'] = test_date['geoNetwork_continent'].apply(lambda x: x if x not in less_continent else 'UnknownContinent')

train_date['geoNetwork_continent'].unique()

"""### Explore sub Continent"""

category_plots('geoNetwork_subContinent')

"""Except North America other subContinets have countable revenue generated

### Explore country
"""

category_plots('geoNetwork_country')



"""### GeoNetwork Network Domain

"""

train_date['geoNetwork_networkDomain'].nunique()

category_plots('geoNetwork_networkDomain')

"""since it has to many values we will reduce it"""

train_date['geoNetwork_networkDomain'].nunique()

print(train_date['geoNetwork_networkDomain'].unique())

popular_networkDomain = train_date['geoNetwork_networkDomain'].value_counts()

#Taking out most popular 100 network domain
popular_networkDomain = popular_networkDomain[:100]

popular_networkDomain

'''
Extracting top 100 Network Domains on basis of value counts and marking remaining as 'unpopular Network Domain' 
For both Test and Train datasets
'''
#train_date['geoNetwork_networkDomain'] = train_date['geoNetwork_networkDomain'].apply(lambda x: x if x in popular_networkDomain else 'unpopularND')
#test_date['geoNetwork_networkDomain'] = test_date['geoNetwork_networkDomain'].apply(lambda x: x if x in popular_networkDomain else 'unpopularND')

train_date['geoNetwork_networkDomain'].nunique()

train_date['geoNetwork_networkDomain'].unique()

"""## **Explore Total columns**"""

total_cols = columns_extract('totals')

train_date[total_cols].nunique(dropna=False)



"""**Explorable columns are totals.hits, totals.pageviews**"""

'''
Function to plot 4 types of barplots for each Totals column
1) Categories vs their counts
2) Categories vs Non-Zero Revenue
3) Categories vs Mean Transaction Revenue
4) Categories vs Total Reveue
'''
def total_col_plots(col):
    a = train_date.loc[:,[col, 'totals_transactionRevenue']]
    a['totals_transactionRevenue'] = a['totals_transactionRevenue'].replace(0.0, np.nan)
    cnt_srs = a.groupby(col)['totals_transactionRevenue'].agg(['size','count','mean'])
    cnt_srs.columns = ["count", 'count of non-zero revenue', "mean"]

    cnt_srs['total_revenue'] = cnt_srs['count of non-zero revenue']*cnt_srs['mean']
    cnt_srs = cnt_srs.sort_values(by="count", ascending=False)
    print(cnt_srs.head(10))
    plt.figure(figsize=(15, 20)) 
    plt.subplot(4,1,1)
    sns.barplot(y=cnt_srs['count'].head(50), x=list(range(1,51)))
    plt.subplot(4,1,2)
    sns.barplot(y=cnt_srs['count of non-zero revenue'].head(50), x=list(range(1,51)))
    plt.subplot(4,1,3)
    sns.barplot(y=cnt_srs['mean'].head(50), x=list(range(1,51)))
    plt.subplot(4,1,4)
    sns.barplot(y=cnt_srs['total_revenue'].head(50), x=list(range(1,51)))

"""### Totals Hits"""

total_col_plots('totals_hits')

"""With increase in page hits revenue increases

###  Totals PageVies Section
"""

total_col_plots('totals_pageviews')

"""With increse in page view, revenue increases

## **Explore Tiraffic sourse analysis**
"""

traffic_cols = columns_extract('trafficSource')

train_date[traffic_cols].nunique(dropna=False)

"""### Traffic Source Campaign"""

category_plots('trafficSource_campaign')

"""### Source"""

category_plots('trafficSource_source')

train_date['trafficSource_source'].unique()

def trafficSourcePopular(x):
    if  ('google' in x):
        return 'google'
    elif  ('youtube' in x):
        return 'youtube'
    elif '(not set)' in x or 'nan' in x:
        return x
    elif 'yahoo' in x:
        return 'yahoo'
    elif 'facebook' in x:
        return 'facebook'
    elif 'reddit' in x:
        return 'reddit'
    elif 'bing' in x:
        return 'bing'
    elif 'quora' in x:
        return 'quora'
    elif 'outlook' in x:
        return 'outlook'
    elif 'linkedin' in x:
        return 'linkedin'
    elif 'pinterest' in x:
        return 'pinterest'
    elif 'ask' in x:
        return 'ask'
    elif 'siliconvalley' in x:
        return 'siliconvalley'
    elif 'lunametrics' in x:
        return 'lunametrics'
    elif 'amazon' in x:
        return 'amazon'
    elif 'mysearch' in x:
        return 'mysearch'
    elif 'qiita' in x:
        return 'qiita'
    elif 'messenger' in x:
        return 'messenger'
    elif 'twitter' in x:
        return 'twitter'
    elif 't.co' in x:
        return 't.co'
    elif 'vk.com' in x:
        return 'vk.com'
    elif 'search' in x:
        return 'search'
    elif 'edu' in x:
        return 'edu'
    elif 'mail' in x:
        return 'mail'
    elif 'ad' in x:
        return 'ad'
    elif 'golang' in x:
        return 'golang'
    elif 'direct' in x:
        return 'direct'
    elif 'dealspotr' in x:
        return 'dealspotr'
    elif 'sashihara' in x:
        return 'sashihara'
    elif 'phandroid' in x:
        return 'phandroid'
    elif 'baidu' in x:
        return 'baidu'
    elif 'mdn' in x:
        return 'mdn'
    elif 'duckduckgo' in x:
        return 'duckduckgo'
    elif 'seroundtable' in x:
        return 'seroundtable'
    elif 'metrics' in x:
        return 'metrics'
    elif 'sogou' in x:
        return 'sogou'
    elif 'businessinsider' in x:
        return 'businessinsider'
    elif 'github' in x:
        return 'github'
    elif 'gophergala' in x:
        return 'gophergala'
    elif 'yandex' in x:
        return 'yandex'
    elif 'msn' in x:
        return 'msn'
    elif 'dfa' in x:
        return 'dfa'
    elif '(not set)' in x:
        return '(not set)'
    elif 'feedly' in x:
        return 'feedly'
    elif 'arstechnica' in x:
        return 'arstechnica'
    elif 'squishable' in x:
        return 'squishable'
    elif 'flipboard' in x:
        return 'flipboard'
    elif 't-online.de' in x:
        return 't-online.de'
    elif 'sm.cn' in x:
        return 'sm.cn'
    elif 'wow' in x:
        return 'wow'
    elif 'baidu' in x:
        return 'baidu'
    elif 'partners' in x:
        return 'partners'
    else:
        return 'others'

#train_date['trafficSource_source'] = train_date['trafficSource_source'].apply(lambda x: trafficSourcePopular(str(x).lower()))
#test_date['trafficSource_source'] = test_date['trafficSource_source'].apply(lambda x: trafficSourcePopular(str(x).lower()))

train_date['trafficSource_source'].nunique()

train_date['trafficSource_source'].unique()

"""### medium"""

category_plots('trafficSource_medium')

train_date['trafficSource_medium'].unique()

"""### adContent"""

category_plots('trafficSource_adContent')

from wordcloud import WordCloud

ad_content = train_date['trafficSource_adContent'].fillna('')
wordcloud2 = WordCloud(width=800, height=400).generate(' '.join(ad_content))
plt.figure( figsize=(12,9))
plt.imshow(wordcloud2)
plt.axis("off")
plt.show()

train_date['trafficSource_adContent'].nunique()

train_date['trafficSource_adContent'].unique()

'''
Funtion to get more revenue generating ads and combining unpopular one
But it is not used considering the fact each ad is different
'''
def popular_adContent(x):
    if  ('google' in x):
        return 'google'
    elif  ('placement' in x) | ('placememnt' in x):
        return 'placement'
    elif '(not set)' in x or 'nan' in x:
        return x
    elif 'ad' in x:
        return x
    else:
        return 'others'

#train_date['trafficSource_adContent'] = train_date['trafficSource_adContent'].apply(lambda x: popular_adContent(str(x).lower()))
#test_date['trafficSource_adContent'] = test_date['trafficSource_adContent'].apply(lambda x: popular_adContent(str(x).lower()))

train_date['trafficSource_adContent'].unique()

"""### Keyword"""

category_plots('trafficSource_keyword')

"""### referral Path"""

category_plots('trafficSource_referralPath')

revenue_generators = ['/', '/yt/about/']

#train_date['trafficSource_referralPath'] = train_date['trafficSource_referralPath'].apply(lambda x : x if x in revenue_generators else 'other')
#test_date['trafficSource_referralPath'] = test_date['trafficSource_referralPath'].apply(lambda x : x if x in revenue_generators else 'other')

train_date.info()

"""# Split categorical and Numerical Columns"""

categorical_cols = list()
for i in train_date.columns:
    if (train_date[i].dtype=='object' or train_date[i].dtype=='bool') and (not(i.startswith('total'))):
        categorical_cols.append(i)

len(categorical_cols)

train_date[categorical_cols].nunique(dropna=False)

"""Numerical columns"""

numerical_cols = list()
for c in train_date.columns:
    if train_date[c].dtype not in ['object', 'bool']:
        numerical_cols.append(c)

train_date[numerical_cols].nunique(dropna = False)

"""Remove fullvisitor ID, session ID before lable encoding"""

categorical_cols.remove('fullVisitorId')
categorical_cols.remove('sessionId')

"""Delete columns which are of no use"""

numerical_cols.remove('date')
numerical_cols.remove('visitId')
numerical_cols.remove('visitStartTime')
#numerical_cols.remove('_year')

numerical_cols

len(numerical_cols) + len(categorical_cols)

"""Check remaining columns"""

set(train_date.columns)-set(numerical_cols + categorical_cols)

numerical_cols.append('totals_bounces')
numerical_cols.append('totals_hits')
numerical_cols.append('totals_newVisits')
numerical_cols.append('totals_pageviews')

#numerical_cols.remove('totals_bounces')
#numerical_cols.remove('totals_newVisits')

numerical_cols



"""# Mising Value Treatment

First lets do for numerical columns
"""

train_date[numerical_cols].isnull().sum()

test_date[numerical_cols].isnull().sum()

train_date['totals_bounces'].unique()

train_date['totals_newVisits'].unique()

train_date['totals_bounces'] = train_date['totals_bounces'].fillna(0)
test_date['totals_bounces'] = test_date['totals_bounces'].fillna(0)

train_date['totals_newVisits'] = train_date['totals_newVisits'].fillna(0)
test_date['totals_newVisits'] = test_date['totals_newVisits'].fillna(0)

train_date['totals_pageviews'] = train_date['totals_pageviews'].fillna(1)
test_date['totals_pageviews'] = test_date['totals_pageviews'].fillna(1)

train_date[numerical_cols].info()

for col in ['totals_hits','totals_pageviews', 'totals_bounces', 'totals_newVisits' ]:
    train_date[col] = train_date[col].astype(int)
    test_date[col] = test_date[col].astype(int)

train_date[numerical_cols].info()

"""Missing values in categorical columns"""

mv = train_date[categorical_cols].isnull().sum()
mp = mv / len(train_date)*100
mp

mvt = test_date[categorical_cols].isnull().sum()
mpt = mvt / len(test_date)*100
mpt

"""percent of missing values in each categorical columns"""

train_date.info()

test_date.info()

#Filling missing values
for col in ['trafficSource_keyword',
            'trafficSource_referralPath',
            'trafficSource_adContent',
            'trafficSource_adwordsClickInfo.page',
            'trafficSource_adwordsClickInfo.slot',
            'trafficSource_adwordsClickInfo.gclId',
            'trafficSource_adwordsClickInfo.adNetworkType',
            'trafficSource_isTrueDirect',
            'trafficSource_adwordsClickInfo.isVideoAd']:
    train_date[col].fillna('unknown', inplace=True)
    test_date[col].fillna('unknown', inplace=True)

"""check for any null value remaining"""

train_date.isnull().sum()

train_date.shape

test_date.shape

train_date.isnull().sum()

train_date.head()

"""# Feature engineering of Total Hits and pageviews"""

train_date['mean_hits_per_networkDomain'] = train_date.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean').astype('int')
train_date['mean_pageViews_per_networkDomain'] = train_date.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean').astype('int')

test_date['mean_hits_per_networkDomain'] = test_date.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean').astype('int')
test_date['mean_pageViews_per_networkDomain'] = test_date.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean').astype('int')

numerical_cols.append('mean_pageViews_per_networkDomain')
numerical_cols.append('mean_hits_per_networkDomain')

train_date.head()

"""# Label Encoding categorical columns"""

#train_copy = train_date.copy()
#test_copy = test_date.copy()

! pip install feature_engine

'''
from feature_engine import categorical_encoders as ce
# set up the encoder
encoder = ce.CountFrequencyCategoricalEncoder(encoding_method='frequency', variables = categorical_cols)

# fit the encoder
encoder.fit(train_date)

# transform the data
train_date = encoder.transform(train_date)
test_date = encoder.transform(test_date)

encoder.encoder_dict_
for col in categorical_cols:
    print("transform column {}".format(col))
    lbe = LabelEncoder()
    lbe.fit(pd.concat([train_date[col], test_date[col]]).astype("str"))
    train_date[col] = lbe.transform(train_date[col].astype("str"))
    test_date[col] = lbe.transform(test_date[col].astype("str"))
    

for col in categorical_cols:
    print("transform column {}".format(col))
    encoder = ce.CountFrequencyCategoricalEncoder(encoding_method='frequency', variables = col)
    # fit the encoder
    encoder.fit(pd.concat([train_date[col], test_date[col]]).astype("str"))
    train_date[col] = encoder.transform(train_date[col].astype("str"))
    test_date[col] = encoder.transform(test_date[col].astype("str"))
    
    
  def frequencyencoding(df,cols):
    #frequency encoding 
    for col in cols:   
    newcol=col
    df[newcol]=df.groupby(col)[col].transform('count')
    df[newcol]=(df[newcol]-df[newcol].min())/(df[newcol].max()-df[newcol].min())
    tenco=pd.DataFrame()
    tenco['target']=target
    #target encoding
    return(df)
'''

train_date.info()

train_date.head()

from sklearn.preprocessing import LabelEncoder
for col in categorical_cols:
    print("transform column {}".format(col))
    lbe = LabelEncoder()
    lbe.fit(pd.concat([train_date[col], test_date[col]]).astype("str"))
    train_date[col] = lbe.transform(train_date[col].astype("str"))
    test_date[col] = lbe.transform(test_date[col].astype("str"))

train_date.head()

test_date.head()

print(train_date.shape, test_date.shape)

"""Remove total_transactionRevenue column from numerical columns as it is to be predicted"""

numerical_cols.remove('totals_transactionRevenue')

#training columns
train_X = train_date[categorical_cols+numerical_cols]
# target column
train_Y = train_date['totals_transactionRevenue']

#test dataset same columns
test_X = test_date[categorical_cols+numerical_cols]

print(categorical_cols + numerical_cols)

train_X.columns

"""### test

"""

train_date['date'].describe()

'''
# Split the train dataset into development and valid based on time
from datetime import date 
dev_df = train_date[train_date['date'].dt.date <= date(2017,5,3)]
val_df = train_date[train_date['date'].dt.date > date(2017,5,3)]
dev_y = np.log1p(dev_df["totals_transactionRevenue"].values)
val_y = np.log1p(val_df["totals_transactionRevenue"].values)

dev_X = dev_df[categorical_cols + numerical_cols] 
val_X = val_df[categorical_cols + numerical_cols] 
test_X = test_date[categorical_cols + numerical_cols]
'''

# Split the train dataset into development and valid based on time
dev_df = train_date
dev_y = np.log1p(dev_df["totals_transactionRevenue"].values)

dev_X = dev_df[categorical_cols + numerical_cols] 
test_X = test_date[categorical_cols + numerical_cols]

train_date

"""# Models

### **GroupKfold + LightGBM**
"""

# custom function to run light gbm model
def kfold_lightgbm(df_x,df_y,test, num_folds, stratified = False, debug= False):
    print('GroupKFold + LGBM')
    
    # Divide in training/validation and test data
    train_df = df_x
    test_df = test
    print("Starting LightGBM. Train shape: {}, test shape: {}".format(train_df.shape, test_df.shape))
    del df_x
    gc.collect()
    # Cross validation model
    if stratified:
        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)
    else:
        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)
    # Create arrays and dataframes to store results
    oof_preds = np.zeros(train_df.shape[0])
    sub_preds = np.zeros(test_df.shape[0])
    feature_importance_df = pd.DataFrame()
    feats = [f for f in train_df.columns ]
    
    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], df_y)):
        train_x, train_y = train_df[feats].iloc[train_idx], df_y[train_idx]
        valid_x, valid_y = train_df[feats].iloc[valid_idx], df_y[valid_idx]
        print('\n Fold : ', n_fold+1)
        # LightGBM parameters found by Bayesian optimization
        clf = LGBMRegressor(
            objective = "regression",
            metric = "rmse", 
            num_leaves = 40,
            min_child_samples = 100,
            learning_rate = 0.005,
            bagging_fraction = 0.85,
            feature_fraction = 0.8,
            bagging_frequency = 6,
            bagging_seed = 42,
            verbosity = -1 )

        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], 
            eval_metric= 'rmse', verbose= 1000, early_stopping_rounds= 500)

        oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)
        sub_preds += clf.predict(test_df[feats], num_iteration=clf.best_iteration_) / folds.n_splits

        fold_importance_df = pd.DataFrame()
        fold_importance_df["feature"] = feats
        fold_importance_df["importance"] = clf.feature_importances_
        fold_importance_df["fold"] = n_fold + 1
        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
        print('Fold %2d RMSE : %.6f' % (n_fold + 1, mean_squared_error(valid_y, oof_preds[valid_idx])))
        del clf, train_x, train_y, valid_x, valid_y
        gc.collect()

    print('Full RMSE score %.6f' % mean_squared_error(df_y, oof_preds))
    # Write submission file and plot feature importance
#    if not debug:
#        test_df['TARGET'] = sub_preds
#        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)
#    display_importances(feature_importance_df)
    return sub_preds

# Training the model #dev_X = dev_df[categorical_cols + numerical_cols] 

#dev_X = train_date[categorical_cols + numerical_cols] 
#dev_y = np.log1p(train_date['totals_transactionRevenue'].values)
#test_X = test_date[categorical_cols + numerical_cols]

pred_test = kfold_lightgbm(dev_X, dev_y, test_X, 6)

#create submission
def createSub(test_id,pred_test):
    sub_df = pd.DataFrame({"fullVisitorId":test_id})
    pred_test[pred_test<0] = 0
    sub_df["PredictedLogRevenue"] = np.expm1(pred_test)
    sub_df = sub_df.groupby("fullVisitorId")["PredictedLogRevenue"].sum().reset_index()
    sub_df.columns = ["fullVisitorId", "PredictedLogRevenue"]
    sub_df["PredictedLogRevenue"] = np.log1p(sub_df["PredictedLogRevenue"])
    sub_df.to_csv("KFold_lgb.csv", index=False)

test_id = test_date["fullVisitorId"].values
    #create submission
createSub(test_id,pred_test)
from google.colab import files
files.download("KFold_lgb.csv")



"""Make proper datasets Train and validation"""

# Split the train dataset into development and valid based on time

dev_df = train_date[train_date['date'].dt.date <= date(2017,5,3)]
val_df = train_date[train_date['date'].dt.date > date(2017,5,3)]
dev_y = np.log1p(dev_df["totals_transactionRevenue"].values)
val_y = np.log1p(val_df["totals_transactionRevenue"].values)

dev_X = dev_df[categorical_cols + numerical_cols] 
val_X = val_df[categorical_cols + numerical_cols] 
test_X = test_date[categorical_cols + numerical_cols]

"""## LGBM"""

# custom function to run light gbm model
import lightgbm as lgb
def run_lgb(train_X, train_y, val_X, val_y, test_X):
    params = {
        "objective" : "regression",
        "metric" : "rmse", 
        "num_leaves" : 40,
        "num_threads" : 10,
        "min_child_samples" : 100,
        "learning_rate" : 0.005,
        "bagging_fraction" : 0.95,
        "feature_fraction" : 0.85,
        "bagging_frequency" : 6,
        "bagging_seed" : 42,
        "verbosity" : -1
    }
    
    lgtrain = lgb.Dataset(train_X, label=train_y)
    lgval = lgb.Dataset(val_X, label=val_y)
    model = lgb.train(params, lgtrain, 10000, valid_sets=[lgval], early_stopping_rounds=500, verbose_eval=500)
    
    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)
    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)
    return pred_test_y, model, pred_val_y

# Training the model #
pred_test, model, pred_val = run_lgb(dev_X, dev_y, val_X, val_y, test_X)

from sklearn import metrics
pred_val[pred_val<0] = 0
val_pred_df = pd.DataFrame({"fullVisitorId":val_df["fullVisitorId"].values})
val_pred_df["transactionRevenue"] = val_df["totals_transactionRevenue"].values
val_pred_df["PredictedRevenue"] = np.expm1(pred_val)
#print(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df["transactionRevenue"].values), np.log1p(val_pred_df["PredictedRevenue"].values))))
val_pred_df = val_pred_df.groupby("fullVisitorId")["transactionRevenue", "PredictedRevenue"].sum().reset_index()
print(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df["transactionRevenue"].values), np.log1p(val_pred_df["PredictedRevenue"].values))))

test_id = test_date["fullVisitorId"].values

sub_df = pd.DataFrame({"fullVisitorId":test_id})
pred_test[pred_test<0] = 0
sub_df["PredictedLogRevenue"] = np.expm1(pred_test)
sub_df = sub_df.groupby("fullVisitorId")["PredictedLogRevenue"].sum().reset_index()
sub_df.columns = ["fullVisitorId", "PredictedLogRevenue"]
sub_df["PredictedLogRevenue"] = np.log1p(sub_df["PredictedLogRevenue"])
sub_df.to_csv("baseline_lgb2.csv", index=False)

sub_df.head()

fig, ax = plt.subplots(figsize=(12,18))
lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)
ax.grid(False)
plt.title("LightGBM - Feature Importance", fontsize=15)
plt.show()

"""## **XGBOOST**"""

import xgboost as xgb

def run_xgb(X_train, y_train, X_val, y_val, X_test):
    params = {'objective': 'reg:linear',
              'eval_metric': 'RMSE',
              'eta': 0.001,
              'max_depth': 10,
              'subsample': 0.6,
              'colsample_bytree': 0.6,
              'alpha':0.001,
              'random_state': 42,
              'silent': True}

    xgb_train_data = xgb.DMatrix(X_train, y_train)
    xgb_val_data = xgb.DMatrix(X_val, y_val)
    xgb_submit_data = xgb.DMatrix(X_test)

    model = xgb.train(params, xgb_train_data, 
                      num_boost_round=2000, 
                      evals= [(xgb_train_data, 'train'), (xgb_val_data, 'valid')],
                      early_stopping_rounds=100, 
                      verbose_eval=500
                     )

    y_pred_train = model.predict(xgb_train_data, ntree_limit=model.best_ntree_limit)
    y_pred_val = model.predict(xgb_val_data, ntree_limit=model.best_ntree_limit)
    y_pred_submit = model.predict(xgb_submit_data, ntree_limit=model.best_ntree_limit)

    return y_pred_submit, model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # SKIP XGB as it takes very much time
# 
# # Train XGBoost and generate predictions
# 
# #xgb_preds, xgb_model = run_xgb(dev_X, dev_y, val_X, val_y, test_X)

"""## **CATBOOST**"""

!pip install catboost

from catboost import CatBoostRegressor

def run_catboost(X_train, y_train, X_val, y_val, X_test):
    model = CatBoostRegressor(iterations=5000,
                             learning_rate=0.01,
                             depth=10,
                             eval_metric='RMSE',
                             random_seed = 42,
                             bagging_temperature = 0.6,
                             od_type='Iter',
                             metric_period = 300,
                             od_wait=150,
                              use_best_model = True)
    model.fit(X_train, y_train,
              eval_set=(X_val, y_val),
              use_best_model=True,
              verbose=True)
    
    y_pred_train = model.predict(X_train)
    y_pred_val = model.predict(X_val)
    y_pred_submit = model.predict(X_test)

    return y_pred_submit, model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Train Catboost and generate predictions
# cat_preds, cat_model = run_catboost(dev_X, dev_y, val_X, val_y, test_X)

# negative values are made zero
pred_test[pred_test<0] = 0
cat_preds[cat_preds<0] = 0
print(pred_test)
print(cat_preds)

# create a dataframe with test's id and target
test_date = pd.DataFrame({"fullVisitorId":test_date["fullVisitorId"].values})

# convert log to exp form for all model predictions
test_date["PredictedRevenue_lgb"] = np.expm1(pred_test)

#test_date["PredictedRevenue_xgb"] = np.expm1(xgb_preds)

test_date["PredictedRevenue_cgb"] = np.expm1(cat_preds)

print(test_date["PredictedRevenue_lgb"])
print(test_date["PredictedRevenue_cgb"])

# find sum of transaction revenue of each user
test_date = test_date.groupby("fullVisitorId")["PredictedRevenue_lgb", "PredictedRevenue_cgb"].sum().reset_index()
len(test_date)

test_date['PredictedRevenue_lgb_log'] = test_date['PredictedRevenue_lgb'].apply(np.log1p)
test_date['PredictedRevenue_cgb_log'] = test_date['PredictedRevenue_cgb'].apply(np.log1p)

ensemble_preds_new= 0.0 * test_date['PredictedRevenue_lgb_log'] + 1.00 * test_date['PredictedRevenue_cgb_log'] 
test_date['ensemble_preds_new'] = ensemble_preds_new
sub_df_3 = pd.DataFrame()
sub_df_3['fullVisitorId'] = test_date['fullVisitorId']
sub_df_3['PredictedLogRevenue'] = test_date['ensemble_preds_new']
sub_df_3.to_csv('submission_cat.csv', index=False)

from google.colab import files
files.download("submission_cat.csv")

"""##Model Ensembling

### LightGBM * 0.7 + CatBoost * 0.3
"""

ensemble_preds_new= 0.70 * test_date['PredictedRevenue_lgb_log'] + 0.30 * test_date['PredictedRevenue_cgb_log'] 
test_date['ensemble_preds_new'] = ensemble_preds_new
sub_df_1 = pd.DataFrame()
sub_df_1['fullVisitorId'] = test_date['fullVisitorId']
sub_df_1['PredictedLogRevenue'] = test_date['ensemble_preds_new']
sub_df_1.to_csv('ensemble_submission_1.csv', index=False)

from google.colab import files
files.download("ensemble_submission_1.csv")

"""### LightGBM * 0.5 + CatBoost * 0.5"""

ensemble_preds_new= 0.50 * test_date['PredictedRevenue_lgb_log'] + 0.50 * test_date['PredictedRevenue_cgb_log'] 
test_date['ensemble_preds_new'] = ensemble_preds_new
sub_df_1 = pd.DataFrame()
sub_df_1['fullVisitorId'] = test_date['fullVisitorId']
sub_df_1['PredictedLogRevenue'] = test_date['ensemble_preds_new']
sub_df_1.to_csv('ensemble_submission_2.csv', index=False)

sub_df_1

from google.colab import files
files.download("ensemble_submission_2.csv")

"""## Using Stratified KFold with XGB algorithm"""

def get_folds(df=None, n_splits=5):
    """Returns dataframe indices corresponding to Visitors stratified KFold"""
    # Get sorted unique visitors
    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))

    # Get folds
    folds = StratifiedKFold(n_splits=n_splits)
    fold_ids = []
    ids = np.arange(df.shape[0])
    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):
        fold_ids.append(
            [
                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],
                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]
            ]
        )

    return fold_ids

"""## Random Forest"""

# Instantiate model with 1000 decision trees
rf = RandomForestRegressor(n_estimators = 1000, criterion='rmse', max_depth=None, min_samples_split=50, min_samples_leaf=1,
                           random_state = 42, verbose= -1 )

'''
n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,
max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, 
n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)



    params = {
        "objective" : "regression",
        "metric" : "rmse", 
        "num_leaves" : 40,
        "num_threads" : 10,
        "min_child_samples" : 100,
        "learning_rate" : 0.005,
        "bagging_fraction" : 0.95,
        "feature_fraction" : 0.85,
        "bagging_frequency" : 6,
        "bagging_seed" : 42,
        "verbosity" : -1
    }
    
    lgtrain = lgb.Dataset(train_X, label=train_y)
    lgval = lgb.Dataset(val_X, label=val_y)
    model = lgb.train(params, lgtrain, 10000, valid_sets=[lgval], early_stopping_rounds=500, verbose_eval=500)
'''


# Train the model on training data
rf.fit(train_features, train_labels);



"""MOdel prediction distribution

Grid Search 

1.   Try different wide range
2.   Then check distribution
3.   All experiments
4.   Correlation
"""